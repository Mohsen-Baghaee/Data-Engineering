{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a69813be-382e-4bdd-a26b-82f2270e656f",
   "metadata": {},
   "source": [
    "# Data Extraction from YouTube API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01e620-a3d2-42b5-81df-f26ea9acafdf",
   "metadata": {},
   "source": [
    "## **PLEASE NOTE**:\n",
    "**The examples used in this code are all public data on YouTube website and the only use case here is just for educational purposes and not commercial.  \n",
    "If a part of data is related to people's opinion like comments in YouTube then the content of the data itself will be not shown in this notebook.  \n",
    "This Tutorial is simply for teaching how to access your desired data and nothing more.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f378c8-5a99-4bfd-80f3-3a466b78be88",
   "metadata": {},
   "source": [
    "In this task the goal is to extract some data from YouTube API, Data can be title of videos of a channel or comments of a specific video with their number of likes and replies or ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf693a1f-0e66-44fc-b8d5-75a5115f201a",
   "metadata": {},
   "source": [
    "For accessing this API first we need to:\n",
    "1. Go to Google Cloud Console\n",
    "2. Make a new project\n",
    "3. In API Library section search for ' YouTube Data API v3 ' and enable it\n",
    "4. Then in API and sercives section --> Credentials --> CREATE CREDENTIALS --> API key \n",
    "5. After creating an API key copy this and go to a jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1238a-c7e3-4900-9874-72dd4c5ff987",
   "metadata": {},
   "source": [
    "Let's start by getting some video titles of a YouTube Channel, for example here from Allianz page.   \n",
    "**Note**: If needed first install googleapiclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2575f05f-9968-4322-b113-9ed1357c337f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of video titles: 50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Diversity, Equity and Inclusion at Allianz',\n",
       " 'Allianz Global Benefits',\n",
       " 'MoveNow Camp – Esports Edition 2023 @ Olympic Esports Week',\n",
       " 'Allianz Financial Results 2Q 2023: Media Conference Call',\n",
       " 'Allianz Financial Results 2Q 2023: Analyst Call',\n",
       " 'Allianz Financial Results 2Q 2023',\n",
       " 'Get ready for that big break',\n",
       " 'Eine Minute zum Ankommen',\n",
       " 'A Minute To Arrive',\n",
       " 'Tomorrow - A podcast by Allianz Research: Walking the talk on green monetary policy',\n",
       " 'The Squared Ball: A symbol for what women must overcome to play professional football',\n",
       " 'Pass The Squared Ball',\n",
       " 'Get ready for a new roommate',\n",
       " 'Get ready to move',\n",
       " 'Future Workout Insurance Intervals – Learning Chapter 1',\n",
       " 'Future Workout Insurance Intervals – Learning Chapter 2',\n",
       " 'Future Workout Insurance Intervals – Learning Chapter 3',\n",
       " 'Allianz Future Workout with Thomas Röhler – Introduction',\n",
       " 'Allianz Future Workout with Thomas Röhler – Starting Blocks',\n",
       " 'Allianz Future Workout with Thomas Röhler – Investment',\n",
       " 'Allianz Future Workout with Thomas Röhler – Insurance',\n",
       " 'Future Workout Starting Blocks – Learning Chapter 1',\n",
       " 'Future Workout Starting Blocks – Learning Chapter 2',\n",
       " 'Future Workout Starting Blocks – Learning Chapter 3',\n",
       " 'Get ready to go from two wheels to four',\n",
       " 'Get ready for life in the water',\n",
       " 'Get ready to train',\n",
       " 'Get ready to travel solo',\n",
       " 'Tomorrow - A podcast by Allianz Research: Allianz Trade Global Survey 2923',\n",
       " 'Get ready for a getaway',\n",
       " 'Tomorrow - A podcast by Allianz Research: Anchor in turbulent times',\n",
       " 'Allianz Global Insurance Report 20231',\n",
       " 'Allianz Financial Results 1Q 2023: Analyst Call',\n",
       " 'Allianz Financial Results 1Q 2023: Media Conference Call',\n",
       " 'Allianz Financial Results 1Q 2023',\n",
       " 'Tomorrow - A podcast by Allianz Research: No quick wins more jobs but little productivity',\n",
       " 'Allianz Financial Results 2022',\n",
       " 'Allianz Hauptversammlung am 4. Mai 2023',\n",
       " 'Allianz Annual General Meeting on May 4, 2023',\n",
       " 'Tomorrow - A podcast by Allianz Research: Reforming against demographic change',\n",
       " 'Tomorrow - A podcast by Allianz Research: business insolvencies',\n",
       " 'Tomorrow: A podcast by Allianz Research - Falling off a savings cliff',\n",
       " 'Apply at Allianz in Record Time',\n",
       " 'The “five Ds” of structurally higher inflation',\n",
       " 'Tomorrow: Taking stock of one year of war economics',\n",
       " 'Allianz Financial Results 2022: Analyst Call',\n",
       " 'Allianz Group Financial Results 2022: Annual Media Conference',\n",
       " 'Allianz Net Zero Interest Income Calculator 2023',\n",
       " 'Start Making Cents – Getting started',\n",
       " 'Start Making Cents – Start early']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the necessary module for making API requests\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# API key obtained from Google Cloud Console, required for authentication\n",
    "api_key = 'YOUR_API_KEY_HERE'  \n",
    "\n",
    "# Create a YouTube API client using the developer key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Construct a request to retrieve the statistics of a YouTube channel\n",
    "request = youtube.channels().list(\n",
    "    part='statistics',  # We want to retrieve statistics data\n",
    "    forUsername='datev'  # Replace with the desired channel's username\n",
    ")\n",
    "\n",
    "# Execute the request and store the response\n",
    "response = request.execute()\n",
    "\n",
    "# Construct a request to retrieve the channel's content details based on its username\n",
    "channel_request = youtube.channels().list(\n",
    "    part='contentDetails',  # We want to retrieve content details\n",
    "    forUsername='Allianz'  # Replace with the desired channel's username\n",
    ")\n",
    "\n",
    "# Execute the channel request and store the response\n",
    "channel_response = channel_request.execute()\n",
    "\n",
    "# Extract the channel ID from the response\n",
    "channel_id = channel_response['items'][0]['id']\n",
    "\n",
    "# Construct a request to retrieve the content details of the channel, including playlists\n",
    "playlist_request = youtube.channels().list(\n",
    "    part='contentDetails',  # We want to retrieve content details\n",
    "    id=channel_id  # Use the extracted channel ID\n",
    ")\n",
    "\n",
    "# Execute the playlist request and store the response\n",
    "playlist_response = playlist_request.execute()\n",
    "\n",
    "# Extract the playlist ID for the uploaded videos from the channel\n",
    "uploads_playlist_id = playlist_response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "\n",
    "# Construct a request to retrieve videos from the uploads playlist\n",
    "videos_request = youtube.playlistItems().list(\n",
    "    part='snippet',  # We want to retrieve video snippet data\n",
    "    playlistId=uploads_playlist_id,  # Use the extracted playlist ID\n",
    "    maxResults=50  # Set the maximum number of results to retrieve\n",
    ")\n",
    "\n",
    "# Execute the videos request and store the response\n",
    "videos_response = videos_request.execute()\n",
    "\n",
    "# Extract video titles from the response\n",
    "video_titles = []\n",
    "for item in videos_response['items']:\n",
    "    video_title = item['snippet']['title']\n",
    "    video_titles.append(video_title)\n",
    "\n",
    "\n",
    "print(f'Number of video titles: {len(video_titles)}')\n",
    "video_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee7f66b-0c6e-40d3-a5c4-60439c49f1b7",
   "metadata": {},
   "source": [
    "# Extracting Comments, Likes and Replies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe8922f-aca7-4ec7-9999-9a64eadd1e57",
   "metadata": {},
   "source": [
    "We need to pick a video on YouTube and put the url of that video into the variable 'video_url' in the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba6dda0f-32bf-4a22-8110-0cd38086fe36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import time\n",
    "import pandas as pd\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "# Replace with your own API key\n",
    "api_key = 'YOUR_API_KEY_HERE'  \n",
    "\n",
    "# Create a YouTube API client using the developer key\n",
    "youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "\n",
    "# Function to retrieve comments from a YouTube video\n",
    "def retrieve_comments(video_id):\n",
    "    comments_data = []  # List to store comments and related data\n",
    "    next_page_token = None  # Token for paginated API responses\n",
    "\n",
    "    while True:\n",
    "        # Construct a request to retrieve comment threads for the video\n",
    "        comments_request = youtube.commentThreads().list(\n",
    "            part='snippet',\n",
    "            videoId=video_id,\n",
    "            textFormat='plainText',\n",
    "            maxResults=100,  # Maximum number of comments per page\n",
    "            pageToken=next_page_token  # Use token for pagination\n",
    "        )\n",
    "        # Execute the comments request and store the response\n",
    "        comments_response = comments_request.execute()\n",
    "\n",
    "        # Iterate through comments in the response\n",
    "        for item in comments_response['items']:\n",
    "            comment_snippet = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = comment_snippet['textDisplay']  # Text of the comment\n",
    "            comment_likes = comment_snippet['likeCount']  # Number of likes on the comment\n",
    "\n",
    "            replies_data = []  # List to store reply texts\n",
    "            # Construct a request to retrieve replies to the current comment\n",
    "            reply_request = youtube.comments().list(\n",
    "                part='snippet',\n",
    "                parentId=item['id'],  # ID of the current comment\n",
    "                maxResults=100  # Maximum number of replies per page\n",
    "            )\n",
    "            # Execute the reply request and store the response\n",
    "            reply_response = reply_request.execute()\n",
    "\n",
    "            # Iterate through replies in the response\n",
    "            for reply_item in reply_response['items']:\n",
    "                reply_text = reply_item['snippet']['textDisplay']\n",
    "                replies_data.append(reply_text)\n",
    "\n",
    "            # Store comment data and related replies in the comments_data list\n",
    "            comments_data.append({\n",
    "                'comment': comment_text,\n",
    "                'likes': comment_likes,\n",
    "                'replies': replies_data\n",
    "            })\n",
    "\n",
    "        # Retrieve the next page token from the response\n",
    "        next_page_token = comments_response.get('nextPageToken')\n",
    "        # Break the loop if there are no more pages or a maximum of 500 comments are retrieved\n",
    "        if not next_page_token or len(comments_data) >= 500:\n",
    "            break\n",
    "\n",
    "        time.sleep(2)  # Add a delay between API requests to avoid rate limits\n",
    "\n",
    "    return comments_data\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    video_url = 'Paste the video url here'\n",
    "    video_id = video_url.split('v=')[1]  # Extract video ID from the URL\n",
    "\n",
    "    # Call the retrieve_comments function to get comments data\n",
    "    comments_data = retrieve_comments(video_id)\n",
    "    \n",
    "    # Create a DataFrame using the comments data\n",
    "    df = pd.DataFrame(comments_data)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Entry point of the script\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a16885-82f8-450e-a009-9af31614a32a",
   "metadata": {
    "tags": []
   },
   "source": [
    "The Result will be store in a DataFrame called 'df' which has 3 columns ['comment', 'likes', 'replies'].  \n",
    "'comment' contains text of each comment  \n",
    "'like' contains the number of likes each comment received  \n",
    "'replies contains' a list of all replies each comment received  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e7268-842d-483a-8bd2-4834d6175969",
   "metadata": {},
   "source": [
    "Finally let's save the DataFrame as a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d172f06-f64b-4842-8be7-e2838f995abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('YouTube_comments_likes_replies.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78a5e40-0849-4071-b73f-d59689e1c8a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "Reference:  \n",
    "https://developers.google.com/youtube/v3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
